{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Recursive Feature Elimination with Cross-Validation (RFECV)"
      ],
      "metadata": {
        "id": "_U4R7S1tmpw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFECV\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = '/content/drive/MyDrive/Machine Learning/scaled_dataset.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "X = df.iloc[:, :-1]  # Features\n",
        "y = df.iloc[:, -1]   # Target\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ---- STEP 1: FILTER METHOD (Mutual Information) ----\n",
        "k = 40  # Select top k features based on mutual information\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
        "X_train_val_selected = selector.fit_transform(X_train_val, y_train_val)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(\"Filter Selected Features:\", selected_features)\n",
        "\n",
        "# ---- STEP 2: WRAPPER METHOD (Recursive Feature Elimination with Cross-Validation) ----\n",
        "base_model = LinearRegression()\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rfecv = RFECV(base_model, step=1, cv=kf, scoring='r2')\n",
        "X_train_val_selected = rfecv.fit_transform(X_train_val_selected, y_train_val)\n",
        "X_test_selected = rfecv.transform(X_test_selected)\n",
        "\n",
        "print(\"Optimal number of features selected:\", rfecv.n_features_)\n",
        "\n",
        "# ---- STEP 3: EMBEDDED METHOD (Lasso Regression for Feature Selection) ----\n",
        "def lasso_feature_selection(X_train, y_train, min_features=20):\n",
        "    alpha = 0.001  # Start with a small alpha\n",
        "    while True:\n",
        "        lasso = Lasso(alpha=alpha)\n",
        "        lasso.fit(X_train, y_train)\n",
        "        important_features = np.where(lasso.coef_ != 0)[0]\n",
        "        if len(important_features) >= min_features:\n",
        "            break  # Stop when we have at least `min_features`\n",
        "        alpha *= 0.5  # Reduce alpha to be less aggressive in feature elimination\n",
        "    return X_train[:, important_features], important_features\n",
        "\n",
        "# Apply Lasso feature selection\n",
        "X_train_val_selected, selected_feature_indices = lasso_feature_selection(X_train_val_selected, y_train_val, min_features=20)\n",
        "X_test_selected = X_test_selected[:, selected_feature_indices]\n",
        "\n",
        "print(\"Final Selected Feature Count:\", len(selected_feature_indices))\n",
        "\n",
        "# Train and evaluate models using K-Fold Cross-Validation\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Polynomial Regression (Degree=2)\": make_pipeline(PolynomialFeatures(2), LinearRegression()),\n",
        "    \"Ridge Regression\": Ridge(),\n",
        "    \"Lasso Regression\": Lasso()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    mae_list, mse_list, rmse_list, r2_list = [], [], [], []\n",
        "    for train_idx, val_idx in kf.split(X_train_val):\n",
        "        X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
        "        y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        mae_list.append(mean_absolute_error(y_val, y_pred))\n",
        "        mse_list.append(mean_squared_error(y_val, y_pred))\n",
        "        rmse_list.append(np.sqrt(mse_list[-1]))\n",
        "        r2_list.append(r2_score(y_val, y_pred))\n",
        "    results[name] = (np.mean(mae_list), np.mean(mse_list), np.mean(rmse_list), np.mean(r2_list))\n",
        "\n",
        "# Display results\n",
        "print(\"\\nModel Performance on Validation Set (K-Fold CV):\")\n",
        "for name, (mae, mse, rmse, r2) in results.items():\n",
        "    print(f\"{name}:\\n  MAE: {mae:.4f}\\n  MSE: {mse:.4f}\\n  RMSE: {rmse:.4f}\\n  R2 Score: {r2:.4f}\\n\")\n",
        "\n",
        "# Select best model (highest average R2 score)\n",
        "best_model_name = max(results, key=lambda x: results[x][3])\n",
        "best_model = models[best_model_name]\n",
        "print(f\"Best Model Before Tuning: {best_model_name}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "Nktb8S0KnCtP",
        "outputId": "0e72e8fc-ddea-4903-e1b9-5cc8c1f8d4a8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filter Selected Features: Index(['MSSubClass', 'LotFrontage', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n",
            "       'BsmtQual', 'GarageYrBlt', 'GarageCars', 'SalePrice', 'TotalBath',\n",
            "       'TotalPorchSF', 'HouseAge', 'TotalSqFt', 'QualitySize',\n",
            "       'TotalBsmtFinScore', 'NeighborhoodQuality', 'ExterQual_ordinal',\n",
            "       'KitchenQual_ordinal', 'LandContour_HLS', 'LandContour_Lvl',\n",
            "       'Condition1_Norm', 'Condition1_RRNn', 'Condition2_PosA',\n",
            "       'BldgType_TwnhsE', 'HouseStyle_2Story', 'HouseStyle_SFoyer',\n",
            "       'Exterior1st_Plywood', 'Exterior1st_Stone', 'Exterior1st_Stucco',\n",
            "       'Exterior1st_VinylSd', 'Exterior2nd_HdBoard', 'Exterior2nd_VinylSd',\n",
            "       'Foundation_CBlock', 'Foundation_PConc', 'Functional_Typ',\n",
            "       'GarageFinish_Unf', 'SaleType_ConLD', 'SaleType_New', 'SaleType_WD',\n",
            "       'SaleCondition_Normal'],\n",
            "      dtype='object')\n",
            "Optimal number of features selected: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-9153f671422e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Apply Lasso feature selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mX_train_val_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_feature_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasso_feature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_val_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mX_test_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_selected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_feature_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-9153f671422e>\u001b[0m in \u001b[0;36mlasso_feature_selection\u001b[0;34m(X_train, y_train, min_features)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mlasso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mlasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mimportant_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportant_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 \u001b[0mthis_Xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m             _, this_coef, this_dual_gap, this_iter = self.path(\n\u001b[0m\u001b[1;32m   1081\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    693\u001b[0m             )\n\u001b[1;32m    694\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mprecompute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             model = cd_fast.enet_coordinate_descent(\n\u001b[0m\u001b[1;32m    696\u001b[0m                 \u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             )\n",
            "\u001b[0;32m_cd_fast.pyx\u001b[0m in \u001b[0;36msklearn.linear_model._cd_fast.enet_coordinate_descent\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/getlimits.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0m_finfo_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finfo_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# most common path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = '/content/drive/MyDrive/Machine Learning/scaled_dataset.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "X = df.iloc[:, :-1]  # Features\n",
        "y = df.iloc[:, -1]   # Target\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ---- STEP 1: FILTER METHOD (Mutual Information) ----\n",
        "k = 40  # Select top k features based on mutual information\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
        "X_train_val_selected = selector.fit_transform(X_train_val, y_train_val)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(\"Filter Selected Features:\", selected_features)\n",
        "\n",
        "# ---- STEP 2: WRAPPER METHOD (Recursive Feature Elimination) ----\n",
        "base_model = LinearRegression()\n",
        "rfe = RFE(base_model, n_features_to_select=20)\n",
        "X_train_val_selected = rfe.fit_transform(X_train_val_selected, y_train_val)\n",
        "X_test_selected = rfe.transform(X_test_selected)\n",
        "\n",
        "# ---- STEP 3: EMBEDDED METHOD (Lasso Regression for Feature Selection) ----\n",
        "# ---- STEP 3: EMBEDDED METHOD (Lasso with Dynamic Alpha) ----\n",
        "def lasso_feature_selection(X_train, y_train, min_features=20):\n",
        "    alpha = 0.001  # Start with a small alpha\n",
        "    while True:\n",
        "        lasso = Lasso(alpha=alpha)\n",
        "        lasso.fit(X_train, y_train)\n",
        "\n",
        "        important_features = np.where(lasso.coef_ != 0)[0]\n",
        "        if len(important_features) >= min_features:\n",
        "            break  # Stop when we have at least `min_features`\n",
        "        alpha *= 0.5  # Reduce alpha to be less aggressive in feature elimination\n",
        "\n",
        "    return X_train[:, important_features], important_features\n",
        "\n",
        "# Apply Lasso feature selection\n",
        "X_train_val_selected, selected_feature_indices = lasso_feature_selection(X_train_val_selected, y_train_val, min_features=20)\n",
        "X_test_selected = X_test_selected[:, selected_feature_indices]\n",
        "\n",
        "print(\"Final Selected Feature Count:\", len(selected_feature_indices))\n",
        "\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Polynomial Regression (Degree=2)\": make_pipeline(PolynomialFeatures(2), LinearRegression()),\n",
        "    \"Ridge Regression\": Ridge(),\n",
        "    \"Lasso Regression\": Lasso()\n",
        "}\n",
        "\n",
        "# Train and evaluate models using K-Fold Cross-Validation\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    mae_list, mse_list, rmse_list, r2_list = [], [], [], []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train_val):\n",
        "        X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
        "        y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        mae_list.append(mean_absolute_error(y_val, y_pred))\n",
        "        mse_list.append(mean_squared_error(y_val, y_pred))\n",
        "        rmse_list.append(np.sqrt(mse_list[-1]))\n",
        "        r2_list.append(r2_score(y_val, y_pred))\n",
        "\n",
        "    results[name] = (np.mean(mae_list), np.mean(mse_list), np.mean(rmse_list), np.mean(r2_list))\n",
        "\n",
        "# Display results\n",
        "print(\"\\nModel Performance on Validation Set (K-Fold CV):\")\n",
        "for name, (mae, mse, rmse, r2) in results.items():\n",
        "    print(f\"{name}:\\n  MAE: {mae:.4f}\\n  MSE: {mse:.4f}\\n  RMSE: {rmse:.4f}\\n  R2 Score: {r2:.4f}\\n\")\n",
        "\n",
        "# Select best model (highest average R2 score)\n",
        "best_model_name = max(results, key=lambda x: results[x][3])\n",
        "best_model = models[best_model_name]\n",
        "print(f\"Best Model Before Tuning: {best_model_name}\\n\")\n",
        "\n",
        "# Hyperparameter tuning for the best model\n",
        "if isinstance(best_model, Ridge) or isinstance(best_model, Lasso):\n",
        "    param_grid = {\"alpha\": [0.01, 0.1, 1, 10, 100]}\n",
        "elif \"Polynomial\" in best_model_name:\n",
        "    param_grid = {\"polynomialfeatures__degree\": [2, 3, 4]}\n",
        "else:\n",
        "    param_grid = {}\n",
        "\n",
        "if param_grid:\n",
        "    grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='r2')\n",
        "    grid_search.fit(X_train_val, y_train_val)\n",
        "    tuned_model = grid_search.best_estimator_\n",
        "\n",
        "    # Training Error\n",
        "    y_train_pred = tuned_model.predict(X_train_val)\n",
        "    train_mae = mean_absolute_error(y_train_val, y_train_pred)\n",
        "    train_mse = mean_squared_error(y_train_val, y_train_pred)\n",
        "    train_rmse = np.sqrt(train_mse)\n",
        "    train_r2 = r2_score(y_train_val, y_train_pred)\n",
        "\n",
        "    # Test Error\n",
        "    y_test_pred = tuned_model.predict(X_test)\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    test_rmse = np.sqrt(test_mse)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    print(\"\\nModel Performance After Tuning:\")\n",
        "    print(f\"{best_model_name} (Training Set):\")\n",
        "    print(f\"  MAE: {train_mae:.4f}\\n  MSE: {train_mse:.4f}\\n  RMSE: {train_rmse:.4f}\\n  R2 Score: {train_r2:.4f}\\n\")\n",
        "\n",
        "    print(f\"{best_model_name} (Test Set):\")\n",
        "    print(f\"  MAE: {test_mae:.4f}\\n  MSE: {test_mse:.4f}\\n  RMSE: {test_rmse:.4f}\\n  R2 Score: {test_r2:.4f}\\n\")\n",
        "\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6obTB_PmWgk",
        "outputId": "46af0512-5058-4297-fee9-9160badd8d14"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filter Selected Features: Index(['YearBuilt', 'YearRemodAdd', 'BsmtQual', 'BedroomAbvGr', 'KitchenAbvGr',\n",
            "       'FireplaceQu', 'GarageYrBlt', 'GarageCars', 'GarageCond', 'SalePrice',\n",
            "       'TotalBath', 'TotalPorchSF', 'HouseAge', 'TotalSqFt', 'QualitySize',\n",
            "       'TotalBsmtFinScore', 'NeighborhoodQuality', 'ExterQual_ordinal',\n",
            "       'HeatingQC_ordinal', 'LotShape_IR3', 'LotConfig_Inside',\n",
            "       'LandSlope_Mod', 'Condition1_PosN', 'Condition2_Feedr',\n",
            "       'HouseStyle_SLvl', 'Exterior1st_Stone', 'Exterior1st_Stucco',\n",
            "       'Exterior1st_VinylSd', 'Exterior2nd_AsphShn', 'Exterior2nd_HdBoard',\n",
            "       'Exterior2nd_Stone', 'Foundation_CBlock', 'Foundation_PConc',\n",
            "       'Heating_Wall', 'Electrical_FuseP', 'Functional_Sev',\n",
            "       'GarageFinish_RFn', 'SaleType_New', 'SaleType_WD',\n",
            "       'SaleCondition_Normal'],\n",
            "      dtype='object')\n",
            "Final Selected Feature Count: 20\n",
            "\n",
            "Model Performance on Validation Set (K-Fold CV):\n",
            "Linear Regression:\n",
            "  MAE: 0.0107\n",
            "  MSE: 0.0015\n",
            "  RMSE: 0.0333\n",
            "  R2 Score: 0.9802\n",
            "\n",
            "Polynomial Regression (Degree=2):\n",
            "  MAE: 0.0518\n",
            "  MSE: 0.0194\n",
            "  RMSE: 0.1367\n",
            "  R2 Score: 0.7340\n",
            "\n",
            "Ridge Regression:\n",
            "  MAE: 0.0130\n",
            "  MSE: 0.0013\n",
            "  RMSE: 0.0309\n",
            "  R2 Score: 0.9826\n",
            "\n",
            "Lasso Regression:\n",
            "  MAE: 0.1527\n",
            "  MSE: 0.0765\n",
            "  RMSE: 0.2751\n",
            "  R2 Score: -0.0082\n",
            "\n",
            "Best Model Before Tuning: Ridge Regression\n",
            "\n",
            "\n",
            "Model Performance After Tuning:\n",
            "Ridge Regression (Training Set):\n",
            "  MAE: 0.0100\n",
            "  MSE: 0.0008\n",
            "  RMSE: 0.0284\n",
            "  R2 Score: 0.9895\n",
            "\n",
            "Ridge Regression (Test Set):\n",
            "  MAE: 0.0148\n",
            "  MSE: 0.0046\n",
            "  RMSE: 0.0681\n",
            "  R2 Score: 0.9442\n",
            "\n",
            "Best Parameters: {'alpha': 1}\n"
          ]
        }
      ]
    }
  ]
}